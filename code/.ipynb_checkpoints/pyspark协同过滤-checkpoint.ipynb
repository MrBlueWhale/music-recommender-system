{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于pyspark的user-based和item-based协同过滤。\n",
    "\n",
    "当数据量大到单机没有办法全部载入的时候，我们就要考虑用spark这种分布式的系统来处理了。让我们看看如何基于pyspark实现user-based和item-based系统过滤算法。\n",
    "\n",
    "要使用spark这种分布式集群，必须要掌握map-reduce的思路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user-based协同过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pdb\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，两个函数完成格式的解析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user item rating timestamp\n",
    "def parse_vector_on_user(line):\n",
    "    \"\"\" 解析数据，key是user，后面是item和打分 \"\"\"\n",
    "    line = line.split('|')\n",
    "    return line[0], (line[1], float(line[2]))\n",
    "\n",
    "def parse_vector_on_item(line):\n",
    "    \"\"\" 解析数据，key是item，后面是user和打分 \"\"\"\n",
    "    line = line.split('|')\n",
    "    return line[1], (line[0], float(line[2]))\n",
    "\n",
    "def sample_interactions(term_id, users_with_rating, n):\n",
    "    \"\"\" 如果某个商品用户行为特别多，可以适当做点下采样 \"\"\"\n",
    "    if len(users_with_rating) > n:\n",
    "        return item_id, random.sample(users_with_rating, n)\n",
    "    else:\n",
    "        return item_id, users_with_rating\n",
    "    \n",
    "def find_user_pairs(item_id, users_with_rating, n):\n",
    "    \"\"\" 对每个item，找到共同打分的user对 \"\"\"\n",
    "    for user1, user2 in combinations(users_with_rating, 2):\n",
    "        return (user1[0], user2[0]), (user1[1], user2[1])\n",
    "\n",
    "def cosine(dot_product, rating_norm_squared, rating2_nrom_squared):\n",
    "    \"\"\" \n",
    "    2个向量A和B的余弦相似度\n",
    "    dotProduct(A, B) / (norm(A) * norm(B))\n",
    "    \"\"\"\n",
    "    numerator = dot_product # 分子\n",
    "    donominator = rating_norm_squared * rating2_nrom_squared # 分母\n",
    "    return (numerator / float(donominator)) if donominator else 0.0\n",
    "\n",
    "def cal_similarity(user_pair, rating_pairs):\n",
    "    \"\"\" 对每个user对，根据打分计算余弦相似度，并返回共同打分的item个数 \"\"\"\n",
    "    sum_xx, sum_xy, sum_yy, sum_x, sum_y, n = (0.0, 0.0, 0.0, 0.0, 0.0, 0)\n",
    "    \n",
    "    for rating_pair in rating_pairs:\n",
    "        sum_xx += np.float(rating_pair[0]) * np.float(rating_pair[0])\n",
    "        sum_yy += np.float(rating_pair[1]) * np.float(rating_pair[1])\n",
    "        sum_xy += np.float(rating_pair[0]) * np.float(rating_pair[1])\n",
    "        n += 1\n",
    "    \n",
    "    cosine_similarity = cosine(sum_xy, np.sqrt(sum_xx), np.sqrt(sum_yy))\n",
    "    return user_pair, (cosine_similarity, n)\n",
    "\n",
    "def key_on_first_user(user_pair, item_similarity_data):\n",
    "    \"\"\" 对每个user-user对，用第一个user做key（map reduce的做法） \"\"\"\n",
    "    (user1_id, user2_id) = user_pair\n",
    "    return user1_id, (user2_id, item_similarity_data)\n",
    "\n",
    "def nearest_neighbors(user, users_and_sims, n):\n",
    "    \"\"\" 选出相似度最高的N个邻居 \"\"\"\n",
    "    users_and_sims.sort(key=lambda x: x[1][0], reverse=True)\n",
    "    return user, users_and_sims[:n]\n",
    "\n",
    "def top_N_recommendations(user_id, user_sims, users_with_rating, n):\n",
    "    \"\"\" 根据最近的N个邻居进行推荐 \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于spark的ALS做推荐系统，针对movielens电影打分数据做推荐。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from operator import add\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rating(line):\n",
    "    \"\"\" movielens的打分格式userId::movieId::rating::timestamp，我们需要先对格式进行解析 \"\"\"\n",
    "    fields = line.strip().split('::')\n",
    "    return long(fields[3]) % 10, (int(fields[0]), int(fields[1]), float(fields[2]))\n",
    "\n",
    "def parse_movie(line): # id => 字符串的映射\n",
    "    \"\"\" 对应的电影文件格式为movieId::movieTitle，解析成id, 文本\"\"\"\n",
    "    fields = line.strip().split('::')\n",
    "    return int(field[0]), fields[1]\n",
    "\n",
    "def load_ratings(ratings_file):\n",
    "    \"\"\" 载入得分 \"\"\"\n",
    "    if not isfile(ratings_file):\n",
    "        print('file %s does not exist' % ratings_file)\n",
    "        sys.exit(1)\n",
    "    f = open(ratings_file, 'r')\n",
    "    # 如果打分比零分小的话，会被过滤掉。\n",
    "    ratings = filter(lambda r: r[2] > 0, [parse_rating(line)[1] for line in f])\n",
    "    f.close()\n",
    "    if not ratings:\n",
    "        print(\"no ratings provided\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return ratings\n",
    "\n",
    "def compute_rmse(model, data, n):\n",
    "    \"\"\" 评估的时候要用的，计算均方根误差 \"\"\"\n",
    "    predictions = model.predictAll(data.map(lambda x: (x[0], x[1])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
